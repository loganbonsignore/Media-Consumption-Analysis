{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from config import nyt_key\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Keyword Mentions With The Natural Language Toolkit\n",
    "* This script tokenizes all headlines, abstracts, lead_paragraphs and keywords into one large string\n",
    "* All stopwords and words containing puncuation or numbers are dropped from the tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_tokenized(year):\n",
    "    months = [\"3\",\"4\",\"5\",\"6\",\"7\"]\n",
    "    \n",
    "    publication_date = []\n",
    "    document_type = []\n",
    "    headline = []\n",
    "    abstract = []\n",
    "    snippet = []\n",
    "    lead_paragraph = []\n",
    "    keyword_1 = []\n",
    "    keyword_2 = []\n",
    "    keyword_3 = []\n",
    "    \n",
    "    for month in months:\n",
    "        base_url = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={nyt_key}\"\n",
    "        response = requests.get(base_url).json()[\"response\"][\"docs\"]\n",
    "        for i in response:\n",
    "            publication_date.append(i[\"pub_date\"])\n",
    "            document_type.append(i[\"document_type\"])\n",
    "            headline.append(i[\"headline\"][\"main\"])\n",
    "            abstract.append(i[\"abstract\"])\n",
    "            lead_paragraph.append(i[\"lead_paragraph\"])\n",
    "\n",
    "    data = {\n",
    "        \"publication_date\":publication_date,\n",
    "        \"document_type\":document_type,\n",
    "        \"headline\":headline,\n",
    "        \"abstract\":abstract,\n",
    "        \"lead_paragraph\":lead_paragraph,\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # converting data points into one large string value\n",
    "    all_headline = df['headline'].str.lower().str.cat(sep=' ')\n",
    "    all_abstract = df['abstract'].str.lower().str.cat(sep=' ')\n",
    "    all_lead_paragraph = df['lead_paragraph'].str.lower().str.cat(sep=' ')\n",
    "    all_words = all_headline + all_abstract + all_lead_paragraph\n",
    "    words_count = all_words.split(sep=\" \")\n",
    "    print(f\"Total number of raw words: {len(words_count):,}\")\n",
    "\n",
    "    # natural language toolkit\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.util import ngrams\n",
    "\n",
    "    # tokenizing the massive string value and filtering out all stopwords, puncuation and numbers\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    word_tokenize=word_tokenize(all_words)\n",
    "    alpha_word_tokenize=[word for word in word_tokenize if word.isalpha()]\n",
    "    filtered_tokenize=[word for word in alpha_word_tokenize if not word in stop_words]\n",
    "    ngram_two=list(ngrams(filtered_tokenize, 2))\n",
    "    filtered_tokenize=ngram_two+filtered_tokenize\n",
    "    print(f\"Total number of tokenized words after filters applied: {len(filtered_tokenize):,}\")\n",
    "\n",
    "    # creating dictionary with keys=keywords and values=number_of_keyword_mentions \n",
    "    term_freq={}\n",
    "    for token in filtered_tokenize: \n",
    "        if token in term_freq: \n",
    "            term_freq[token]+=1\n",
    "        else: \n",
    "            term_freq[token]=1\n",
    "\n",
    "    # getting the top 100 mentions of all headlines, abstracts, lead_paragraphs and keywords\n",
    "    import math\n",
    "    sort_freq=sorted(term_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_terms_freq=sort_freq[:]\n",
    "    top_terms_dict={}\n",
    "    for each_term_freq in top_terms_freq: \n",
    "        if type(each_term_freq[0])==tuple: \n",
    "            top_terms_dict[' '.join(each_term_freq[0])]=each_term_freq[1]\n",
    "        else: \n",
    "            top_terms_dict[each_term_freq[0]]=each_term_freq[1]\n",
    "            \n",
    "    series = pd.Series(top_terms_dict,index=top_terms_dict.keys())\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of raw words: 2,355,868\n",
      "Total number of tokenized words after filters applied: 2,646,945\n"
     ]
    }
   ],
   "source": [
    "series_2020 = pull_tokenized(2020)\n",
    "series_2016 = pull_tokenized(2016)\n",
    "series_2012 = pull_tokenized(2012)\n",
    "series_2008 = pull_tokenized(2008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coronavirus          14456\n",
       "new                  10989\n",
       "pandemic              6454\n",
       "trump                 6062\n",
       "one                   5691\n",
       "                     ...  \n",
       "defibrillator            1\n",
       "indistinguishably        1\n",
       "yanking                  1\n",
       "modus                    1\n",
       "operandi                 1\n",
       "Length: 618005, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Counts\n",
    "#### This DF contains the counts of each unique word mentioned in the NYT's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2020</th>\n",
       "      <th>2016</th>\n",
       "      <th>2012</th>\n",
       "      <th>2008</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>14456.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>10989.0</td>\n",
       "      <td>17045.0</td>\n",
       "      <td>24894.0</td>\n",
       "      <td>27846.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pandemic</th>\n",
       "      <td>6454.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>6062.0</td>\n",
       "      <td>6727.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>5691.0</td>\n",
       "      <td>6631.0</td>\n",
       "      <td>10153.0</td>\n",
       "      <td>14286.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kickier</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rattan</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ketchupy</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chitarra</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fleecing</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3435618 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                2020     2016     2012     2008\n",
       "coronavirus  14456.0      NaN      NaN      NaN\n",
       "new          10989.0  17045.0  24894.0  27846.0\n",
       "pandemic      6454.0      7.0     11.0      7.0\n",
       "trump         6062.0   6727.0     92.0    132.0\n",
       "one           5691.0   6631.0  10153.0  14286.0\n",
       "...              ...      ...      ...      ...\n",
       "kickier          NaN      NaN      NaN      1.0\n",
       "rattan           NaN      NaN      NaN      1.0\n",
       "ketchupy         NaN      NaN      NaN      1.0\n",
       "chitarra         NaN      NaN      NaN      1.0\n",
       "fleecing         NaN      NaN      NaN      1.0\n",
       "\n",
       "[3435618 rows x 4 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([series_2020,series_2016,series_2012,series_2008],axis=1)\n",
    "df = df.rename(columns={df.columns[0]:\"2020\",df.columns[1]:\"2016\",df.columns[2]:\"2012\",df.columns[3]:\"2008\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combined.to_csv(\"Output/DFs/tokens_raw.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
